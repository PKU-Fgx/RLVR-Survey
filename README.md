# Awesome LLM-RLVR [ğŸ”¥ğŸ“œ]

<div align="center">
  <a href="https://github.com/PKU-Fgx/RLVR-Survey/stargazers"><img src="https://img.shields.io/github/stars/PKU-Fgx/RLVR-Survey?style=for-the-badge" alt="Stargazers"></a>
  <a href="https://github.com/PKU-Fgx/RLVR-Survey/network/members"><img src="https://img.shields.io/github/forks/sPKU-Fgx/RLVR-Survey?style=for-the-badge" alt="Forks"></a>
  <a href="https://github.com/PKU-Fgx/RLVR-Survey/graphs/contributors"><img src="https://img.shields.io/github/contributors/PKU-Fgx/RLVR-Survey?style=for-the-badge" alt="Contributors"></a>
  <a href="https://github.com/PKU-Fgx/RLVR-Survey/blob/main/LICENSE"><img src="https://img.shields.io/github/license/PKU-Fgx/RLVR-Survey?style=for-the-badge" alt="MIT License"></a>
</div>

>  ä¸€ä»½ç²¾é€‰çš„ç ”ç©¶è®ºæ–‡ã€å·¥å…·ã€æ•°æ®é›†å’Œæ¡†æ¶åˆ—è¡¨ï¼Œç”¨äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­å…·æœ‰**å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ **ï¼ˆReinforcement Learning with Verifiable Rewards, RLVRï¼‰ã€‚
> å—åŸºç¡€æ¨¡å‹åœ¨å¯¹é½ã€æ¨ç†ä¸è‡ªæˆ‘æ”¹è¿›ä¸‰è€…äº¤å‰é¢†åŸŸçš„å¯å‘ã€‚ 

<details>
  <summary>ğŸ—‚ï¸ ç›®å½• </summary>
  <ol>
    <li><a href="#motivation">ğŸŒŸ Motivation</a></li>
    <li><a href="#auto-fetched-recent-papers">ğŸ”„ Auto-Fetched Recent Papers</a></li>
    <li><a href="#core-papers">ğŸ§  Core Papers</a></li>
    <li><a href="#unsupervised-rlvr">ğŸ” Unsupervised RLVR</a></li>
    <li><a href="#entropy-in-rlvr">ğŸ“‰ Entropy in RLVR</a></li>
    <li><a href="#rlvr-analysis">ğŸ§ª RLVR Analysis</a></li>
    <li><a href="#mllm">ğŸ–¼ï¸ Multimodal LLM</a></li>
    <li><a href="#evaluation">ğŸ“ Evaluation Issue</a></li>
    <li><a href="#blogs">ğŸ“° Awesome Blogs about RLVR</a></li>
    <li><a href="#toolkits-and-libraries">ğŸ› ï¸ Toolkits and Libraries</a></li>
    <li><a href="#star">â­ Star History</a></li>
    <li><a href="#contributing">ğŸ¤ Contributing</a></li>
    <li><a href="#license">ğŸ§¾ License</a></li>
  </ol>
</details>

---

<h2 id="motivation">ğŸŒŸ Motivation</h2>

RLVR æ˜¯ä¸€ç§å¿«é€Ÿå‘å±•çš„èŒƒå¼ï¼Œé€šè¿‡å¤–éƒ¨å¥–åŠ±éªŒè¯ã€è‡ªæ´½æ€§å’Œè‡ªä¸¾å­¦ä¹ æ¥å¯¹é½å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨ä¸è¿‡åº¦ä¾èµ–äººå·¥ç›‘ç£çš„æƒ…å†µä¸‹æå‡æ¨ç†èƒ½åŠ›ã€‚

---

### ğŸ”„ Auto-Fetched Recent Papers



<h2 id="core-papers">ğŸ§  Core Papers</h2>

<h3 id="rlvr-foundations">RLVR Foundations</h3>

<h4>2025</h4>

1. ğŸ‘‰ã€é«˜æ•ˆæ¨ç†ã€‘**Optimizing Anytime Reasoning via Budget Relative Policy Optimization.**   <2025.06>      
    [[Paper]](https://arxiv.org/abs/2505.13438)    *Sea AIã€æ–°åŠ å¡å›½ç«‹*   
    > åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°æ¡†æ¶ AnytimeReasonerï¼Œç”¨äºä¼˜åŒ–â€œéšæ—¶æ¨ç†â€ï¼ˆanytime reasoningï¼‰æ€§èƒ½ï¼Œç›®æ ‡æ˜¯åœ¨ä¸åŒ token é¢„ç®—çº¦æŸä¸‹æé«˜ token æ•ˆç‡å’Œæ¨ç†çµæ´»æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªå…ˆéªŒåˆ†å¸ƒä¸­é‡‡æ · token é¢„ç®—ï¼Œå¹¶å°†å®Œæ•´çš„æ€ç»´è¿‡ç¨‹æˆªæ–­åˆ°è¿™äº›é¢„ç®—é•¿åº¦ï¼Œè¿«ä½¿æ¨¡å‹ä¸ºæ¯æ®µæˆªæ–­çš„æ€ç»´ç”Ÿæˆå¯éªŒè¯çš„æœ€ä½³ç­”æ¡ˆï¼Œä»¥æ­¤å¼•å…¥å¯éªŒè¯çš„å¯†é›†å¥–åŠ±ï¼Œä»è€Œåœ¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ä¸­å®ç°æ›´æœ‰æ•ˆçš„ä¿¡ç”¨åˆ†é…ã€‚
   
2. ğŸ‘‰ã€æ”¹è¿› RLVR èŒƒå¼ã€‘**Language Models Can Learn from Verbal Feedback Without Scalar Rewards.**   <2025.09>      
    [[Paper]](https://arxiv.org/abs/2509.22638)    *Sea AI*   
    > æˆ‘ä»¬æå‡ºå°†å£å¤´åé¦ˆè§†ä¸ºä¸€ç§æ¡ä»¶ä¿¡å·ï¼Œè€Œä¸æ˜¯å‹ç¼©ä¸ºå•ä¸€çš„æ•°å€¼å¥–åŠ±ã€‚

3. ğŸ‘‰ã€é«˜æ•ˆæ¨ç†ã€‘**Entropy After $\textlangle$ `/Think` $\textrangle$ for reasoning model early exiting.**   <2025.09>      
    [[Paper]](https://arxiv.org/abs/2509.26522)    *Netflix Research*   
    > ä¸ºæ£€æµ‹å¹¶é˜²æ­¢è¿‡åº¦æ€è€ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ä¸”å»‰ä»·çš„ä¿¡å· â€”â€” â€œ`</Think>` ä¹‹åçš„ç†µï¼ˆEntropy After `</Think>`, EATï¼‰â€ â€”â€” ç”¨äºç›‘æ§å¹¶å†³å®šæ˜¯å¦æå‰é€€å‡ºæ¨ç†ã€‚
    
4. ğŸ‘‰ã€RLVR åŸç†ã€‘**Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective.**  <2025.06>      
    [[Paper]](https://www.arxiv.org/abs/2506.02553)    *Amazon*   
   > åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç»Ÿä¸€çš„ç†è®ºè§†è§’ï¼Œæˆ‘ä»¬å¼•å…¥äº†**è½¨è¿¹ç­–ç•¥æ¢¯åº¦å®šç†ï¼ˆTrajectory Policy Gradient Theoremï¼‰**ï¼Œè¡¨æ˜æ— è®ºé›¶å¥–åŠ±å‡è®¾æ˜¯å¦æˆç«‹ï¼Œåœ¨ REINFORCE å’Œ Actor-Critic ä¸€ç±»ç®—æ³•ä¸­ï¼Œåªç”¨ æ•´æ¡å›åº”çš„æœ€ç»ˆå¥–åŠ±ï¼ˆresponse-level rewardï¼‰ å°±å¯ä»¥å¯¹çœŸå®ã€æœªçŸ¥çš„ token-çº§å¥–åŠ±è¿›è¡Œæ— åä¼°è®¡ã€‚

5. ğŸ‘‰ã€æ•°æ®é«˜æ•ˆã€‘**Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward.**  <2025.09>      
    [[Paper]](https://www.arxiv.org/abs/2509.01321)    *äººå¤§ã€é˜¿é‡Œèš‚èš*   
   > è¿‘æœŸçš„å¤§å‹æ¨ç†æ¨¡å‹ç ”ç©¶åˆ©ç”¨å¸¦å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLVRï¼‰æ¥æå‡æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ç„¶è€Œï¼Œè¿™ç±»æ–¹æ³•è¦åšåˆ°å¤§è§„æ¨¡è®­ç»ƒé€šå¸¸éœ€è¦å¤§é‡çš„ rollout è®¡ç®—ä»¥åŠå·¨å¤§çš„æ•°æ®é›†ï¼Œä»è€Œå¯¼è‡´è®­ç»ƒæˆæœ¬é«˜ã€æ•°æ®æ•ˆç‡ä½ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬è®ºæ–‡æå‡ºäº† DEPOï¼ˆData-Efficient Policy Optimizationï¼‰ â€”â€” ä¸€ç§æ•°æ®é«˜æ•ˆçš„ç­–ç•¥ä¼˜åŒ–ç®¡é“ï¼Œå®ƒç»“åˆäº†ç¦»çº¿å’Œåœ¨çº¿æ•°æ®é€‰æ‹©çš„ä¼˜åŒ–ç­–ç•¥ã€‚

6. ğŸ‘‰ã€æ”¹è¿› RLVR èŒƒå¼ã€‘**Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts.** <2025.06>   
   [[Paper]](https://www.arxiv.org/abs/2506.02177) [[Code]](https://github.com/Infini-AI-Lab/GRESO/)    *Carnegie Mellonã€Meta*  
   > å°½ç®¡å¸¦æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning with Verifiable Rewards, RLVRï¼‰å·²æˆä¸ºå‘å±•å¤§å‹è¯­è¨€æ¨¡å‹é«˜çº§æ¨ç†èƒ½åŠ›çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½†ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œåœ¨ç»è¿‡æ•°åƒæ¬¡ä¼˜åŒ–æ­¥éª¤åè®­ç»ƒä¼šå‡ºç°æ€§èƒ½åœæ»ï¼ˆå³â€œå¹³å°æœŸâ€ï¼‰ï¼Œå³ä½¿å¢åŠ è®¡ç®—èµ„æºä¹Ÿéš¾ä»¥è·å¾—æ˜¾è‘—æ€§èƒ½æå‡ã€‚é€ æˆè¿™ä¸€é™åˆ¶çš„åŸå› æ˜¯å½“å‰ RLVR è®­ç»ƒä¸­æ¢ç´¢éå¸¸ç¨€ç– â€”â€” æ¨¡å‹ä¾èµ–æœ‰é™çš„ rolloutï¼ˆæ‰§è¡Œè½¨è¿¹ï¼‰ç»å¸¸é”™è¿‡å…³é”®çš„æ¨ç†è·¯å¾„ï¼Œæ— æ³•ç³»ç»Ÿè¦†ç›–è§£ç©ºé—´ã€‚   
   > æˆ‘ä»¬æå‡ºäº† DeepSearchï¼Œä¸€ä¸ªå°†**è’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMonte Carlo Tree Search, MCTS**ç›´æ¥é›†æˆè¿› RLVR è®­ç»ƒçš„æ¡†æ¶ã€‚ä¸ç°æœ‰æ–¹æ³•ä»…åœ¨æ¨ç†ï¼ˆinferenceï¼‰è¿‡ç¨‹ä¸­ä½¿ç”¨æ ‘æœç´¢ä¸åŒï¼ŒDeepSearch æŠŠç»“æ„åŒ–æœç´¢åµŒå…¥è®­ç»ƒå¾ªç¯ï¼Œå®ç°äº†é’ˆå¯¹æ¨ç†æ­¥éª¤çš„ç³»ç»Ÿæ€§æ¢ç´¢å’Œç»†ç²’åº¦çš„å¥–åŠ±ï¼ˆcreditï¼‰åˆ†é…ã€‚   

7. ğŸ‘‰ã€æ”¹è¿› RLVR èŒƒå¼ã€‘**Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning.** <2025.06>   
   [[Paper]](https://www.arxiv.org/abs/2510.05251)    *University of Chicagoã€Meta*  
   > åœ¨æœ¬æ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§æ›´ç®€å•ä¸”æ›´æœ‰æ•ˆçš„ç­–ç•¥â€”â€”æ¢ç´¢æ€§é€€ç«è§£ç ï¼ˆExploratory Annealed Decoding, EADï¼‰ã€‚è¯¥æ–¹æ³•åŸºäºä¸€ä¸ªæ ¸å¿ƒæ´å¯Ÿï¼š**åœ¨åºåˆ—ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ—©æœŸ token å¯¹æ•´ä¸ªåºåˆ—çš„è¯­ä¹‰æ–¹å‘æœ€å…³é”®ï¼Œå› æ­¤æ¢ç´¢æ€§åœ¨æ—©æœŸæ›´æœ‰ä»·å€¼ã€‚**    
     EAD é‡‡ç”¨äº†â€œå…ˆæ¢ç´¢ã€ååˆ©ç”¨ï¼ˆexplore-at-the-beginning, exploit-at-the-endï¼‰â€çš„åŠ¨æ€æ¸©åº¦ç­–ç•¥â€”â€”ä»è¾ƒé«˜æ¸©åº¦é€æ­¥é€€ç«åˆ°è¾ƒä½æ¸©åº¦ã€‚  

8. ğŸ‘‰ã€æ”¹è¿› GRPOã€‘**KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning.** <2025.06>  
    [[Paper]](https://arxiv.org/pdf/2506.02208v1)    *å“ˆå·¥å¤§*
   >æœ¬æ–‡æå‡ºäº† KDRLï¼Œä¸€ç§ ç»Ÿä¸€çš„åè®­ç»ƒæ¡†æ¶ï¼Œé€šè¿‡æ•™å¸ˆç›‘ç£ï¼ˆKDï¼‰å’Œæ¨¡å‹è‡ªèº«æ¢ç´¢ï¼ˆRLï¼‰å…±åŒä¼˜åŒ–æ¨ç†æ¨¡å‹ã€‚å…·ä½“è€Œè¨€ï¼ŒKDRL ä½¿ç”¨ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼ŒåŒæ—¶æœ€å°åŒ–å­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹åˆ†å¸ƒçš„é€†å‘ Kullbackâ€“Leibler æ•£åº¦ï¼ˆRKLï¼‰ï¼Œå¹¶æœ€å¤§åŒ–åŸºäºè§„åˆ™çš„é¢„æœŸå¥–åŠ±ã€‚è®ºæ–‡æ„å»ºäº†ä¸€ä¸ªå°† GRPOï¼ˆGroup Relative Policy Optimizationï¼‰ ä¸ KD ç»“åˆçš„ç»Ÿä¸€ç›®æ ‡ï¼Œå¹¶ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†ä¸åŒ KL è¿‘ä¼¼æ–¹æ³•ã€KL æƒé‡ç³»æ•°ä»¥åŠåŸºäºå¥–åŠ±æŒ‡å¯¼çš„ KD ç­–ç•¥å¯¹è®­ç»ƒåŠ¨æ€å’Œæ€§èƒ½çš„å½±å“ã€‚
   
9. ğŸ‘‰ã€æ”¹è¿› GRPOã€‘**Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening.** <2025.06>  
    [[Paper]](https://www.arxiv.org/abs/2506.02355)    *Carnegie Mellon University.*  
   > ä¸ºäº†è§£å†³ GRPO çš„æ’åºåå·®ï¼Œæˆ‘ä»¬æå‡ºäº† unlikeliness rewardï¼ˆä¸å¤ªå¯èƒ½æ€§å¥–åŠ±ï¼‰ï¼Œä¸€ç§æ˜¾å¼æé«˜ä½æ¦‚ç‡æ­£ç¡®ç»“æœæƒé‡çš„æ–¹æ³•ã€‚æˆ‘ä»¬å±•ç¤ºäº†ä¸å¤ªå¯èƒ½æ€§å¥–åŠ±å¯ä»¥ç¼“è§£æ’åºåå·®ï¼Œå¹¶åœ¨åˆæˆå’ŒçœŸå®çš„å®šç†è¯æ˜è®¾ç½®ä¸­ï¼Œåœ¨å¤šä¸ª N å€¼ä¸‹æé«˜ pass@N è¡¨ç°ã€‚
   
10. ğŸ‘‰ã€RLVR æ˜¯å¦èƒ½æ‹“å±•èƒ½åŠ›è¾¹ç•Œã€‘**The Invisible Leash: Why RLVR May or May Not Escape Its Origin.** <2025.09>  
    [[Paper]](https://www.arxiv.org/abs/2507.14843)    *Stanford Universityã€NVIDIA*  
   > æœ¬ç ”ç©¶é€šè¿‡ç†è®ºå’Œå¤§è§„æ¨¡å®è¯ç ”ç©¶æ­ç¤ºäº† RLVR å¯èƒ½çš„å±€é™æ€§ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨å½“å‰è®­ç»ƒæ¡ä»¶ä¸‹ï¼ŒRLVR çš„ä¼˜åŒ–å¾€å¾€åƒä¸€ç§ å—åŸºåº§æ¨¡å‹æ”¯æŒé›†é™åˆ¶çš„ä¿å®ˆæ€§é‡åŠ æƒæœºåˆ¶ï¼Œè¿™å¯èƒ½é™åˆ¶å®Œå…¨åŸåˆ›è§£çš„å‘ç°ï¼Œå¹¶ä½¿ä¼˜åŒ–å—é™äºåŸºåº§æ¨¡å‹çš„åˆå§‹åˆ†å¸ƒã€‚æˆ‘ä»¬è¿˜è¯†åˆ«å‡ºä¸€ä¸ª ç†µä¸å¥–åŠ±ä¹‹é—´çš„æƒè¡¡ï¼šè™½ç„¶ç°æœ‰çš„ RLVR æ–¹æ¡ˆå¯é åœ°æå‡äº†ç²¾åº¦ï¼Œä½†å®ƒä¹Ÿå¯èƒ½é€æ­¥ç¼©å°æ¢ç´¢èŒƒå›´ï¼Œä»è€Œé—æ¼é‚£äº›æ­£ç¡®ä½†æ¦‚ç‡è¾ƒä½çš„è§£ã€‚

11.  **Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback** <2025.06>  
    *Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, Helen Meng* **arXiv**    
   [[Paper]](https://www.arxiv.org/abs/2506.03106) [[Code]](https://github.com/zhangxy-2019/critique-GRPO)
12. **Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning.** <2025.05>  
   *Shenao Zhang, Yaqing Wang, Yinxiao Liu, Tianqi Liu, Peter Grabowski, Eugene Ie, Zhaoran Wang, Yunxuan Li.* **arXiv**  
    [[Paper]](https://arxiv.org/abs/2505.20561)
13. **S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models.** <2025.05>  
    *Muzhi Dai, Chenxu Yang, Qingyi Si.* **arXiv**  
   [[Paper]](https://arxiv.org/abs/2505.07686)
14. **Temporal Sampling for Forgotten Reasoning in LLMs** <2025.05>   
   *Yuetai Li, Zhangchen Xu, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Xiang Yue, Radha Poovendran.* **arXiv**    
   [[Paper]](https://arxiv.org/abs/2505.20196) [[Code]](https://github.com/uw-nsl/Temporal_Forgetting)
15. **DAPO: An Open-Source LLM Reinforcement Learning System at Scale.**  <2025.05>   
   *Yu et al.* **arXiv**    
   [[Paper]](https://arxiv.org/abs/2503.14476) [[Code]](https://github.com/BytedTsinghua-SIA/DAPO)
16. **RM-R1: Reward Modeling as Reasoning**  <2025.05>  
    *Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, Hanghang Tong, Heng Ji.* **arXiv**  
    [[Paper]](https://arxiv.org/abs/2505.02387) [[Code]](https://github.com/RM-R1-UIUC/RM-R1)
17. **GRPO-LEAD: A Difficulty-Aware Reinforcement Learning Approach for Concise Mathematical Reasoning in Language Models.**  <2025.04>   
    *Jixiao Zhang, Chunsheng Zuo.* **arXiv**   
    [[Paper]](https://arxiv.org/abs/2504.09696)
18. **VAPO: Efficient and Reliable Reinforcement Learning for Advanced Reasoning Tasks.** <2025.04>
    *Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, Lin Yan.* **arXiv**
    [Paper]()
19. **Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning.**  <2025.04>   
    *Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, Donghyun Kwak.*  **arXiv**   
    [[Paper]](https://arxiv.org/abs/2504.03380)
20. **SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement Learning on LLM.** <2025.04>   
    *Xiaojiang Zhang, Jinghui Wang, Zifei Cheng, Wenhao Zhuang, Zheng Lin, Minglei Zhang, Shaojie Wang, Yinghan Cui, Chao Wang, Junyi Peng, Shimiao Jiang, Shiqi Kuang, Shouyu Yin, Chaohang Wen, Haotian Zhang, Bin Chen, Bing Yu.*  **arXiv**   
    [[Paper]](https://arxiv.org/abs/2504.14286)
21. **DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.**  <2025.01>  
    *DeepSeek-AI.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2501.12948)
22. **ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models.**  <2025.05>  
    *Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, Yi Dong.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2505.24864)
23. **GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning.**  <2025.07>  
    *Ziru Liu, Cheng Gong, Xinyu Fu, Yaofang Liu, Ran Chen, Shoubo Hu, Suiyun Zhang, Rui Liu, Qingfu Zhang, Dandan        Tu.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2507.10628)
24. **GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning.**  <2025.04>  
    *Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, Yong Wang.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2504.02546)
25. **SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data.**  <2025.05>  
    *Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kaixuan Chen, Mingli Song, Dacheng Tao.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2505.20347)
26. **SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning.**  <2025.06>  
    *Ruiqi Zhang, Daman Arora, Song Mei, Andrea Zanette.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2506.09016)
27. **Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning.**  <2025.06>  
    *Hanbing Liu, Lang Cao, Yuanyi Ren, Mengyu Zhou, Haoyu Dong, Xiaojun Ma, Shi Han, Dongmei Zhang.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2506.08125)
28. **Reinforcement Pre-Training.**  <2025.06>  
    *Qingxiu Dong, Li Dong, Yao Tang, Tianzhu Ye, Yutao Sun, Zhifang Sui, Furu Wei.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2506.08007)
29. **Reinforcement Learning with Verifiable Rewards: GRPOâ€™s Effective Loss, Dynamics, and Success Amplification.**  <2025.03>  
    *Youssef Mroueh.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2503.06639)

<h4>2024</h4>

1. **DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models** <2024.02>  
   *Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo.* **arXiv**   
    [[Paper]](https://arxiv.org/abs/2402.03300) [[Code]](https://github.com/deepseek-ai/DeepSeek-Math)
2. **ReFT: Reasoning with Reinforced Fine-Tuning** <2024.01>  
   *Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li.* **arXiv / ACL 2024**  
   [[Paper]](https://arxiv.org/abs/2401.08967) [[Code]](https://github.com/lqtrung1998/mwp_ReFT)


---

<h3 id="rlvr-foundations">Unsupervised RLVR</h3>

<h4>2025</h4>

1. **Can Large Reasoning Models Self-Train?** <2025.05> 
    *Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, Andrea Zanette.* **arXiv**   
    [[Paper]](https://arxiv.org/abs/2505.21444) 
2. **Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization. **    <2025.05>   
    *Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, Yatao Bian.*     **arXiv**
    [[Paper]](https://arxiv.org/abs/2504.05812)  
4.  **Maximizing Confidence Alone Improves Reasoning**  <2025.05>     
    *Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, Deepak Pathak.*   **arXiv**  
    [[Paper]](https://arxiv.org/abs/2505.22660)
5.  **Learning to Reason without External Rewards.**   **arXiv**
    *Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, Dawn Song.*      **arXiv**  
    [[Paper]](https://arxiv.org/abs/2505.19590)
6. **NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning.**  <2025.05>  
    *Wei Liu, Siya Qi, Xinyu Wang, Chen Qian, Yali Du, Yulan He.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2505.16022)
7. **Absolute Zero: Reinforced Self-Play Reasoning with Zero Data.**  <2025.05>  
    *Andrew Zhao et al.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2505.03335)
---

<h3 id="rlvr-foundations">Entropy in RLVR</h3>

<h4>2025</h4>

1. **Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning** <2025.06>  
    *Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, Junyang Lin.* **arXiv**    
    [[Paper]](https://arxiv.org/abs/2506.01939)
2. **The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models.** <2025.05>   
    *Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, Ning Ding.* **arXiv**    
    [[Paper]](https://arxiv.org/abs/2505.22617) [[Code]](https://github.com/PRIME-RL/Entropy-Mechanism-of-RL)
3. **One-shot Entropy Minimization.** <2025.05>  
    *Zitian Gao, Lynx Chen, Joey Zhou, Bryan Dai.* **arXiv**  
   [[Paper]](https://www.arxiv.org/pdf/2505.20282) [[Code]](https://github.com/zitian-gao/one-shot-em)
4. **The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning.** <2025.05>  
   *Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, Hao Peng.* **arXiv**  
   [[Paper]](https://arxiv.org/abs/2505.15134)
5. **FR3E: First Return, Entropy-Eliciting Explore.**  <2025.07>  
   *Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, Qian Liu, Ge Zhang, Zejun Ma*  **arXiv**  
   [[Paper]](https://arxiv.org/abs/2507.07017)
6. **Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models.**  <2025.08>  
   *Zhipeng Chen, Xiaobo Qin, Youbin Wu, Yue Ling, Qinghao Ye, Wayne Xin Zhao, Guang Shi.*  **arXiv**  
   [[Paper]](https://arxiv.org/abs/2508.10751)
---

<h3 id="rlvr-analysis">RLVR Analysis</h3>

<h4>2025</h4>

1. **Spurious Rewards: Rethinking Training Signals in RLVR.**  <2025.05>  
    *Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, Luke Zettlemoyer.*  **Preprint**  
    [[Paper]](https://github.com/ruixin31/Rethink_RLVR/blob/main/paper/rethink-rlvr.pdf) [[Code]](https://github.com/ruixin31/Rethink_RLVR)
2. **Reinforcement Learning Finetunes Small Subnetworks in Large Language Models.**  <2025.05>  
    *Sagnik Mukherjee, Lifan Yuan, Dilek Hakkani-Tur, Hao Peng.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2505.11711)
3. **Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?**  <2025.04>  
    *Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, Gao Huang.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2504.13837) [[Code]](https://github.com/LeapLabTHU/limit-of-RLVR)
4. **The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning.**  <2025.06>  
    *Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, Yu Meng.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2506.01347) [[Code]](https://github.com/TianHongZXY/RLVR-Decomposed)
5. **Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination.**  <2025.07>  
    *Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Huijie Lv, Ming Zhang, Yanwei Fu, Qin Liu, Songyang Zhang, Qi Zhang.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2507.10532)
6. **One Token to Fool LLM-as-a-Judge.**  <2025.07>  
    *Yulai Zhao, Haolin Liu, Dian Yu, S. Y. Kung, Haitao Mi, Dong Yu.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2507.08794)

---
<h3 id="mllm">Multimodal LLM</h3>

<h4>2025</h4>

1. *SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning.***  <2025.06> 
    *Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, Yangfan He, Mi Zhang, Shen Yan.*   **arXiv**   
    [[Paper]](https://arxiv.org/abs/2506.01713) [[Code]](https://srpo.pages.dev/)
2. **R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning** <2025.05>   
    *Jiaxing Zhao, Xihan Wei, Liefeng Bo*  **arXiv**   
    [[Paper]](https://arxiv.org/abs/2503.05379) [[Code]](https://github.com/HumanMLLM/R1-Omni)
3. **R1-VL: Learning to Reason with Multimodal Large Language Models via Step-Wise GRPO.**  <2025.03>  
   *Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, Dacheng Tao.*  **arXiv**  
   [[Paper]](https://arxiv.org/abs/2503.12937)
4. **VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning.**  <2025.04>  
   *Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Wenhu Chen.*  **arXiv**  
   [[Paper]](https://arxiv.org/abs/2504.08837)
5. **GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training.**  <2025.03>  
   *Tong Wei, Yijun Yang, Junliang Xing, Yuanchun Shi, Zongqing Lu, Deheng Ye.*  **arXiv**  
   [[Paper]](https://arxiv.org/abs/2503.08525)

---
<h3 id="evaluation">Evaluation Issue </h3>

<h4>2025</h4>

1. **A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility.**  <2025.04>  
    *Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, Matthias Bethge.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2504.07086)
2. **Incorrect Baseline Evaluations Call into Question Recent LLM-RL Claims.**  <2025.05>  
    *Nikhil Chandak, Shashwat Goel, Ameya Prabhu.*  **Blog**  
    [[Paper]](https://safe-lip-9a8.notion.site/Incorrect-Baseline-Evaluations-Call-into-Question-Recent-LLM-RL-Claims-2012f1fbf0ee8094ab8ded1953c15a37)
3. **Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design.**  <2025.06>  
    *Lin Sun, Weihong Lin, Jinzhu Wu, Yongfu Zhu, Xiaoqi Jian, Guangxiang Zhao, Linglin Zhang, Sai-er Hu, Yuhan Wu, Xiangzheng Zhang.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2506.04734)
4. **AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions.**  <2025.06>  
    *Polina Kirichenko, Shauli Ravfogel, Roee Aharoni, Yonatan Belinkov.*  **arXiv**  
    [[Paper]](https://arxiv.org/abs/2506.09038)
5. **IFBench: Generalizing Verifiable Instruction Following.**  <2025.07>  
   *Valentina Pyatkin, Saumya Malik, Victoria Graf, Hamish Ivison, Shengyi Huang, Pradeep Dasigi, Nathan Lambert, Hannaneh Hajishirzi.*  **arXiv**  
   [[Paper]](https://arxiv.org/abs/2507.02833) [[Code]](https://github.com/allenai/if-bench)
 
--- 

<h2 id="blogs">ğŸ“š Awesome Blogs about RLVR</h2>

[SemiAnalysis](https://semianalysis.com/2025/06/08/scaling-reinforcement-learning-environments-reward-hacking-agents-scaling-data/): 
Scaling Reinforcement Learning: Environments, Reward Hacking, Agents, Scaling Data.    


---

[//]: # ([//]: # &#40;&#41;)
[//]: # ([//]: # &#40;<h2 id="surveys-and-theory">ğŸ“š Surveys and Theory</h2>&#41;)
[//]: # ()
[//]: # ()
[//]: # (<h2 id="datasets-and-benchmarks">ğŸ—ï¸ Datasets and Benchmarks</h2>)

[//]: # ()
[//]: # (---)

<h2 id="toolkits-and-libraries">ğŸ› ï¸ Toolkits and Libraries</h2>

- ã€[open-r1](https://github.com/huggingface/open-r1)ã€‘-- Fully open reproduction of DeepSeek-R1. ![GitHub Stars](https://img.shields.io/github/stars/huggingface/open-r1?style=social)
- ã€[PRIME](https://github.com/PRIME-RL/PRIME)ã€‘ -- Scalable RL solution for advanced reasoning of language models.  ![GitHub Stars](https://img.shields.io/github/stars/PRIME-RL/PRIME?style=social)
- ã€[simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason)ã€‘ -- Simple RL training for reasoning.  ![GitHub Stars](https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason?style=social)
- ã€[TinyZero](https://github.com/Jiayi-Pan/TinyZero)ã€‘ -- Minimal reproduction of DeepSeek R1-Zero. ![GitHub Stars](https://img.shields.io/github/stars/Jiayi-Pan/TinyZero?style=social)
- ã€[OpenR](https://github.com/openreasoner/openr)ã€‘ -- OpenR: An Open Source Framework for Advanced Reasoning with LLMs![GitHub Stars](https://img.shields.io/github/stars/openreasoner/openr?style=social)
- ã€[verl](https://github.com/volcengine/verl)ã€‘ -- verl: Volcano Engine Reinforcement Learning for LLMs. ![GitHub Stars](https://img.shields.io/github/stars/volcengine/verl?style=social)
- ã€[rl](https://github.com/pytorch/rl)ã€‘ -- A modular, primitive-first, python-first PyTorch library for Reinforcement Learning. ![GitHub Stars](https://img.shields.io/github/stars/pytorch/rl?style=social)
- ã€[all-rl-algorithms](https://github.com/FareedKhan-dev/all-rl-algorithms)ã€‘ -- Implementation of all RL algorithms in a simpler way. ![GitHub Stars](https://img.shields.io/github/stars/FareedKhan-dev/all-rl-algorithms?style=social)
- ã€[AReaL](https://github.com/inclusionAI/AReaL)ã€‘ -- Distributed RL System for LLM Reasoning. ![GitHub Stars](https://img.shields.io/github/stars/inclusionAI/AReaL?style=social)
- ã€[rllm](https://github.com/agentica-project/rllm)ã€‘ -- Democratizing Reinforcement Learning for LLMs. ![GitHub Stars](https://img.shields.io/github/stars/agentica-project/rllm?style=social)
- ã€[MARTI](https://github.com/TsinghuaC3I/MARTI)ã€‘ -- A Framework for LLM-based Multi-Agent Reinforced Training and Inference.  ![GitHub Stars](https://img.shields.io/github/stars/TsinghuaC3I/MARTI?style=social)
- ã€[OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) ã€‘-- An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray. ![GitHub Stars](https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?style=social)
- ã€[ROLL](https://github.com/alibaba/ROLL)ã€‘-- An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models. ![GitHub Stars](https://img.shields.io/github/stars/alibaba/ROLL?style=social)

<h2>ğŸ’¡ Other Awesome Lists</h2>

- **[Awesome-RL-based-LLM-Reasoning](https://github.com/bruno686/Awesome-RL-based-LLM-Reasoning)** Materials that enhance LLM reasoning with reinforcement learning. 
- **[Awesome-LLM-Reasoning](https://github.com/atfortes/Awesome-LLM-Reasoning)**  Curated collection of papers and resources on how to unlock the reasoning ability of LLMs and MLLMs.
- **[Awesome-Controllable-Generation](https://github.com/atfortes/Awesome-Controllable-Generation)**  Collection of papers and resources on Controllable Generation using Diffusion Models.
- **[Chain-of-ThoughtsPapers](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers)**  A trend starting from "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models".
- **[LM-reasoning](https://github.com/jeffhj/LM-reasoning)**  Collection of papers and resources on Reasoning in Large Language Models.
- **[Prompt4ReasoningPapers](https://github.com/zjunlp/Prompt4ReasoningPapers)**  Repository for the paper "Reasoning with Language Model Prompting: A Survey".
- **[ReasoningNLP](https://github.com/FreedomIntelligence/ReasoningNLP)**  Paper list on reasoning in NLP.
- **[Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)**  Curated list of Large Language Model.
- **[Awesome LLM Self-Consistency](https://github.com/SuperBruceJia/Awesome-LLM-Self-Consistency)**  Curated list of Self-consistency in Large Language Models.
- **[Deep-Reasoning-Papers](https://github.com/floodsung/Deep-Reasoning-Papers)**  Recent Papers including Neural-Symbolic Reasoning, Logical Reasoning, and Visual Reasoning.

<p align="right" style="font-size: 14px; color: #555; margin-top: 20px;">
    <a href="#top" style="text-decoration: none; color: #007bff; font-weight: bold;">
        â†‘ Back to Top â†‘
    </a>
</p>
